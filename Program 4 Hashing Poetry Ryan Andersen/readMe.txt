Humans have a vast pool of knowledge of all the subtle nuances required to synthesize speech. For almost all humans,
this knowledge is aquired effortlessley. Some examples of these nuances that the computer isn't aware of includes:
1. Sentences need to follow a grammatical structure (for example, nouns, verbs, adjectives, in specific order)
2. These formatted sentences then need to convey a thought
3. This thought needs to be able to be interpreted by a reader
The computer has no idea what grammar is, what a thought is, and has no concept of what can be understood.
An improvement in any of these areas would significantly increase the sensibility of its output. A pattern I noticed was that this
program had more comprehensible output when the input had more unique words, or when the words were more generic. It makes sense
that more unique input created more comprehensible output, because it means the program was more likely to spit out something
that resempbled the original poem (as there were fewer choices to follow each word.) In the other case, it naturally follows that
generic words (things like pronouns and verbs) are able to be more flexible in their use.

